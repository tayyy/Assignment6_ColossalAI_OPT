{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSYAddx3Aohz","executionInfo":{"status":"ok","timestamp":1712752378911,"user_tz":-480,"elapsed":615,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"}},"outputId":"ca0c51fc-c001-4029-f7fe-6793996030c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Assignment6_ColossalAI_OPT'...\n","remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects:   5% (1/19)\u001b[K\rremote: Compressing objects:  10% (2/19)\u001b[K\rremote: Compressing objects:  15% (3/19)\u001b[K\rremote: Compressing objects:  21% (4/19)\u001b[K\rremote: Compressing objects:  26% (5/19)\u001b[K\rremote: Compressing objects:  31% (6/19)\u001b[K\rremote: Compressing objects:  36% (7/19)\u001b[K\rremote: Compressing objects:  42% (8/19)\u001b[K\rremote: Compressing objects:  47% (9/19)\u001b[K\rremote: Compressing objects:  52% (10/19)\u001b[K\rremote: Compressing objects:  57% (11/19)\u001b[K\rremote: Compressing objects:  63% (12/19)\u001b[K\rremote: Compressing objects:  68% (13/19)\u001b[K\rremote: Compressing objects:  73% (14/19)\u001b[K\rremote: Compressing objects:  78% (15/19)\u001b[K\rremote: Compressing objects:  84% (16/19)\u001b[K\rremote: Compressing objects:  89% (17/19)\u001b[K\rremote: Compressing objects:  94% (18/19)\u001b[K\rremote: Compressing objects: 100% (19/19)\u001b[K\rremote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 19 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects:   5% (1/19)\rReceiving objects:  10% (2/19)\rReceiving objects:  15% (3/19)\rReceiving objects:  21% (4/19)\rReceiving objects:  26% (5/19)\rReceiving objects:  31% (6/19)\rReceiving objects:  36% (7/19)\rReceiving objects:  42% (8/19)\rReceiving objects:  47% (9/19)\rReceiving objects:  52% (10/19)\rReceiving objects:  57% (11/19)\rReceiving objects:  63% (12/19)\rReceiving objects:  68% (13/19)\rReceiving objects:  73% (14/19)\rReceiving objects:  78% (15/19)\rReceiving objects:  84% (16/19)\rReceiving objects:  89% (17/19)\rReceiving objects:  94% (18/19)\rReceiving objects: 100% (19/19)\rReceiving objects: 100% (19/19), 11.86 KiB | 11.86 MiB/s, done.\n","Resolving deltas:   0% (0/4)\rResolving deltas:  25% (1/4)\rResolving deltas:  50% (2/4)\rResolving deltas:  75% (3/4)\rResolving deltas: 100% (4/4)\rResolving deltas: 100% (4/4), done.\n","/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\n"]}],"source":["!git clone https://github.com/tayyy/Assignment6_ColossalAI_OPT.git\n","%cd Assignment6_ColossalAI_OPT"]},{"cell_type":"code","source":["!set -xe\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1H98cHd1all","executionInfo":{"status":"ok","timestamp":1712751880648,"user_tz":-480,"elapsed":6994,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"}},"outputId":"aeea2814-2b63-4a4c-8fae-ef7bc0906b58"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: colossalai>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.6)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.18.0)\n","Requirement already satisfied: transformers>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.38.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (24.0)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (1.11.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.4.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.6.4)\n","Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.10.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.1.99)\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r requirements.txt (line 2)) (12.4.127)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->-r requirements.txt (line 4)) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->-r requirements.txt (line 4)) (0.15.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.2.2)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.2.14)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.12.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 2)) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.5.35)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (20.25.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.16.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.0.8)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.1.2)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (42.0.5)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.5)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.18.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.22)\n"]}]},{"cell_type":"code","source":["\n","\n","# model name or path\n","MODEL=\"facebook/opt-350m\"\n","\n","# path for saving model\n","OUTPUT_PATH=\"./output_model.bin\"\n","\n","# plugin(training strategy)\n","# can only be one of \"torch_ddp\"/\"torch_ddp_fp16\"/\"low_level_zero\"/\"gemini\"\n","PLUGIN=\"gemini\"\n","\n","# number of gpus to use\n","GPUNUM=1\n","\n","# batch size per gpu\n","BS=16\n","\n","# learning rate\n","LR=\"5e-5\"\n","\n","# number of epoch\n","EPOCH=10\n","\n","# weight decay\n","WEIGHT_DECAY=0.01\n","\n","# ratio of warmup steps\n","WARMUP_RATIO=0.1\n","\n","# run the script for demo\n","!colossalai run \\\n","--nproc_per_node {GPUNUM} \\\n","opt_train_demo.py \\\n","--model_name_or_path {MODEL} \\\n","--output_path {OUTPUT_PATH} \\\n","--plugin {PLUGIN} \\\n","--batch_size {BS} \\\n","--num_epoch {EPOCH} \\\n","--learning_rate {LR} \\\n","--weight_decay {WEIGHT_DECAY} \\\n","--warmup_ratio {WARMUP_RATIO}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfmT8nlTAvqe","outputId":"3720dda4-a702-4025-c3a1-68e999d634a6","executionInfo":{"status":"ok","timestamp":1712758578274,"user_tz":-480,"elapsed":5879627,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 12:38:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 12:38:27] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:86 main\n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-350m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:113    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09548354148864746 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10950374603271484 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 12:38:37] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:145    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n","Epoch [1]:   0%|          | 0/550 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","Epoch [1]: 100%|██████████| 550/550 [09:38<00:00,  1.05s/it, loss=1.64]\n","Epoch [2]: 100%|██████████| 550/550 [09:38<00:00,  1.05s/it, loss=1.32]\n","Epoch [3]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=1.08]\n","Epoch [4]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=0.881]\n","Epoch [5]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=0.688]\n","Epoch [6]: 100%|██████████| 550/550 [09:51<00:00,  1.07s/it, loss=0.521]\n","Epoch [7]: 100%|██████████| 550/550 [09:46<00:00,  1.07s/it, loss=0.391]\n","Epoch [8]: 100%|██████████| 550/550 [09:52<00:00,  1.08s/it, loss=0.318]\n","Epoch [9]: 100%|██████████| 550/550 [09:56<00:00,  1.08s/it, loss=0.277]\n","Epoch [10]: 100%|██████████| 550/550 [09:56<00:00,  1.08s/it, loss=0.267]\n","[04/10/24 14:16:07] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:150    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n","[04/10/24 14:16:13] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:152    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n","                             ./output_model.bin                                                     \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}]},{"cell_type":"code","source":["MEMCAP=0\n","GPUNUM=1\n","for BS in [8, 32, 128]:\n","    for PLUGIN in [\"torch_ddp\", \"torch_ddp_fp16\", \"low_level_zero\", \"gemini\"]:\n","        MODLE_PATH=\"facebook/opt-125m\"\n","        !colossalai run \\\n","        --nproc_per_node {GPUNUM} \\\n","        opt_benchmark.py \\\n","        --model_name_or_path {MODLE_PATH} \\\n","        --mem_cap {MEMCAP} \\\n","        --plugin {PLUGIN} \\\n","        --batch_size {BS}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVU_7psDAxd-","executionInfo":{"status":"ok","timestamp":1712759410791,"user_tz":-480,"elapsed":475549,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"}},"outputId":"63fbd5c1-495f-4012-ef2b-5915aeb67754"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:22:23] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","config.json: 100%|██████████| 651/651 [00:00<00:00, 2.89MB/s]\n","[04/10/24 14:22:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10634350776672363 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.13509154319763184 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:22:28] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [01:01<00:00,  3.15s/it][04/10/24 14:23:29] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main\n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp, throughput: 2.5927, maximum memory usage per gpu:\n","                             8.38 GB.                                                               \n","Training Step: 100%|██████████| 20/20 [01:01<00:00,  3.09s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:23:42] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:23:45] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0995786190032959 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11594939231872559 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:23:46] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:28<00:00,  1.42s/it][04/10/24 14:24:15] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main\n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp_fp16, throughput: 5.5551, maximum memory usage per\n","                             gpu: 7.11 GB.                                                          \n","Training Step: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:24:27] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:24:30] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10286569595336914 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.13617849349975586 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:24:31] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.25s/it][04/10/24 14:24:57] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main\n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: low_level_zero, throughput: 6.1991, maximum memory usage per\n","                             gpu: 4.90 GB.                                                          \n","Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:25:11] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:25:15] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.11539578437805176 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.14260029792785645 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 14:25:18] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:29<00:00,  1.42s/it][04/10/24 14:25:47] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main\n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: gemini, throughput: 5.3969, maximum memory usage per gpu:   \n","                             3.51 GB.                                                               \n","Training Step: 100%|██████████| 20/20 [00:29<00:00,  1.48s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:26:00] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:26:04] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09829092025756836 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.12272787094116211 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:26:05] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n","    else self._run_ddp_forward(*inputs, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n","    return self.module(*inputs, **kwargs)  # type: ignore[index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1164, in forward\n","    shift_logits = logits[..., :-1, :].contiguous()\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.13 GiB. GPU 0 has a total capacity of 14.75 GiB of which 4.72 GiB is free. Process 386290 has 10.03 GiB memory in use. Of the allocated memory 8.52 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:04<?, ?it/s]\n","[2024-04-10 14:26:11,894] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31821) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:26:11\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 31821)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp --batch_size 32 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp --batch_size 32'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:26:20] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:26:23] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09930205345153809 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.12273001670837402 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:26:24] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/booster/mixed_precision/fp16_torch.py\", line 93, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n","    else self._run_ddp_forward(*inputs, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n","    return self.module(*inputs, **kwargs)  # type: ignore[index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1168, in forward\n","    loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3059, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.13 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.18 GiB is free. Process 387924 has 13.56 GiB memory in use. Of the allocated memory 11.87 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:02<?, ?it/s]\n","[2024-04-10 14:26:32,001] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31946) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:26:32\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 31946)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp_fp16 --batch_size 32 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp_fp16 --batch_size 32'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:26:40] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:26:43] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09728288650512695 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1138918399810791 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:26:44] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step:   5%|▌         | 1/20 [00:06<01:54,  6.02s/it]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/booster/plugin/low_level_zero_plugin.py\", line 65, in forward\n","    return super().forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1168, in forward\n","    loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3059, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 14.75 GiB of which 205.06 MiB is free. Process 388842 has 14.54 GiB memory in use. Of the allocated memory 11.58 GiB is allocated by PyTorch, and 2.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   5%|▌         | 1/20 [00:07<02:20,  7.39s/it]\n","[2024-04-10 14:26:56,305] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32061) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:26:56\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 32061)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin low_level_zero --batch_size 32 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin low_level_zero --batch_size 32'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:27:04] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:27:07] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09936690330505371 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1177670955657959 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 14:27:10] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [01:40<00:00,  4.92s/it][04/10/24 14:28:51] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main\n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             32, plugin: gemini, throughput: 6.3529, maximum memory usage per gpu:  \n","                             13.25 GB.                                                              \n","Training Step: 100%|██████████| 20/20 [01:40<00:00,  5.04s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:29:04] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:29:07] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10971236228942871 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.12118101119995117 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:29:08] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n","    else self._run_ddp_forward(*inputs, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n","    return self.module(*inputs, **kwargs)  # type: ignore[index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1145, in forward\n","    outputs = self.model.decoder(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 901, in forward\n","    layer_outputs = self._gradient_checkpointing_func(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24, in inner\n","    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 482, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 261, in forward\n","    outputs = run_function(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 552, in forward\n","    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 232, in forward\n","    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 3.94 GiB is free. Process 397827 has 10.80 GiB memory in use. Of the allocated memory 10.07 GiB is allocated by PyTorch, and 299.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]\n","[2024-04-10 14:29:14,996] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32727) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:29:14\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 32727)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp --batch_size 128 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp --batch_size 128'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:29:23] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:29:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Time taken to compile cpu_adam_x86 op: 0.16248250007629395 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.13934993743896484 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:29:28] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/booster/mixed_precision/fp16_torch.py\", line 93, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n","    else self._run_ddp_forward(*inputs, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n","    return self.module(*inputs, **kwargs)  # type: ignore[index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1145, in forward\n","    outputs = self.model.decoder(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 901, in forward\n","    layer_outputs = self._gradient_checkpointing_func(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24, in inner\n","    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 482, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 261, in forward\n","    outputs = run_function(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 552, in forward\n","    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 233, in forward\n","    attn_weights = torch.max(\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 4.44 GiB is free. Process 398969 has 10.30 GiB memory in use. Of the allocated memory 9.51 GiB is allocated by PyTorch, and 355.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:01<?, ?it/s]\n","[2024-04-10 14:29:34,598] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32844) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:29:34\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 32844)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp_fp16 --batch_size 128 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin torch_ddp_fp16 --batch_size 128'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:29:43] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:29:46] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10314488410949707 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11774206161499023 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 14:29:47] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/booster/plugin/low_level_zero_plugin.py\", line 65, in forward\n","    return super().forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/interface/model.py\", line 25, in forward\n","    return self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1145, in forward\n","    outputs = self.model.decoder(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 901, in forward\n","    layer_outputs = self._gradient_checkpointing_func(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24, in inner\n","    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 482, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 261, in forward\n","    outputs = run_function(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 552, in forward\n","    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 240, in forward\n","    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1860, in softmax\n","    ret = input.softmax(dim, dtype=dtype)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 3.35 GiB is free. Process 400082 has 11.39 GiB memory in use. Of the allocated memory 5.47 GiB is allocated by PyTorch, and 5.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:01<?, ?it/s]\n","[2024-04-10 14:29:49,747] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32963) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:29:49\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 32963)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin low_level_zero --batch_size 128 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin low_level_zero --batch_size 128'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 14:29:58] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 14:30:01] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.1045980453491211 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.12420201301574707 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 14:30:04] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 130, in <module>\n","    main()\n","  File \"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_benchmark.py\", line 106, in main\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids, use_cache=False)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/gemini_ddp.py\", line 272, in forward\n","    outputs = self.module(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 1145, in forward\n","    outputs = self.model.decoder(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 901, in forward\n","    layer_outputs = self._gradient_checkpointing_func(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24, in inner\n","    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 482, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 261, in forward\n","    outputs = run_function(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 552, in forward\n","    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 240, in forward\n","    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1854, in softmax\n","    return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/overrides.py\", line 1621, in handle_torch_function\n","    result = torch_func_method(public_api, types, args, kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/colossalai/tensor/colo_tensor.py\", line 91, in __torch_function__\n","    ret = func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1860, in softmax\n","    ret = input.softmax(dim, dtype=dtype)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 3.63 GiB is free. Process 400813 has 11.12 GiB memory in use. Of the allocated memory 5.01 GiB is allocated by PyTorch, and 5.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Training Step:   0%|          | 0/20 [00:01<?, ?it/s]\n","[2024-04-10 14:30:09,622] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 33060) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","opt_benchmark.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-04-10_14:30:09\n","  host      : cb66d0c2b27c\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 33060)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n","Error: failed to run torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin gemini --batch_size 128 on 127.0.0.1, is localhost: True, exception: Encountered a bad command exit code!\n","\n","Command: 'cd /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT && export SHELL=\"/bin/bash\" NV_LIBCUBLAS_VERSION=\"12.2.5.6-1\" NVIDIA_VISIBLE_DEVICES=\"all\" COLAB_JUPYTER_TRANSPORT=\"ipc\" NV_NVML_DEV_VERSION=\"12.2.140-1\" NV_CUDNN_PACKAGE_NAME=\"libcudnn8\" CGROUP_MEMORY_EVENTS=\"/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\" NV_LIBNCCL_DEV_PACKAGE=\"libnccl-dev=2.19.3-1+cuda12.2\" NV_LIBNCCL_DEV_PACKAGE_VERSION=\"2.19.3-1\" VM_GCE_METADATA_HOST=\"169.254.169.253\" HOSTNAME=\"cb66d0c2b27c\" LANGUAGE=\"en_US\" TBE_RUNTIME_ADDR=\"172.28.0.1:8011\" GCE_METADATA_TIMEOUT=\"3\" NVIDIA_REQUIRE_CUDA=\"cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\" NV_LIBCUBLAS_DEV_PACKAGE=\"libcublas-dev-12-2=12.2.5.6-1\" NV_NVTX_VERSION=\"12.2.140-1\" COLAB_JUPYTER_IP=\"172.28.0.12\" NV_CUDA_CUDART_DEV_VERSION=\"12.2.140-1\" NV_LIBCUSPARSE_VERSION=\"12.1.2.141-1\" COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=\"http://172.28.0.1:8013/\" NV_LIBNPP_VERSION=\"12.2.1.4-1\" NCCL_VERSION=\"2.19.3-1\" KMP_LISTEN_PORT=\"6000\" TF_FORCE_GPU_ALLOW_GROWTH=\"true\" ENV=\"/root/.bashrc\" PWD=\"/content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT\" TBE_EPHEM_CREDS_ADDR=\"172.28.0.1:8009\" COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=\"30s\" TBE_CREDS_ADDR=\"172.28.0.1:8008\" NV_CUDNN_PACKAGE=\"libcudnn8=8.9.6.50-1+cuda12.2\" NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\" LAST_FORCED_REBUILD=\"20240404\" NV_NVPROF_DEV_PACKAGE=\"cuda-nvprof-12-2=12.2.142-1\" NV_LIBNPP_PACKAGE=\"libnpp-12-2=12.2.1.4-1\" NV_LIBNCCL_DEV_PACKAGE_NAME=\"libnccl-dev\" TCLLIBPATH=\"/usr/share/tcltk/tcllib1.20\" NV_LIBCUBLAS_DEV_VERSION=\"12.2.5.6-1\" COLAB_KERNEL_MANAGER_PROXY_HOST=\"172.28.0.12\" NVIDIA_PRODUCT_NAME=\"CUDA\" NV_LIBCUBLAS_DEV_PACKAGE_NAME=\"libcublas-dev-12-2\" USE_AUTH_EPHEM=\"1\" NV_CUDA_CUDART_VERSION=\"12.2.140-1\" COLAB_WARMUP_DEFAULTS=\"1\" HOME=\"/root\" LANG=\"en_US.UTF-8\" COLUMNS=\"100\" CUDA_VERSION=\"12.2.2\" CLOUDSDK_CONFIG=\"/content/.config\" NV_LIBCUBLAS_PACKAGE=\"libcublas-12-2=12.2.5.6-1\" NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=\"cuda-nsight-compute-12-2=12.2.2-1\" COLAB_RELEASE_TAG=\"release-colab_20240408-060136_RC00\" PYDEVD_USE_FRAME_EVAL=\"NO\" KMP_TARGET_PORT=\"9000\" CLICOLOR=\"1\" KMP_EXTRA_ARGS=\"--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-30w0demntvxr1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\" NV_LIBNPP_DEV_PACKAGE=\"libnpp-dev-12-2=12.2.1.4-1\" COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=\"/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\" NV_LIBCUBLAS_PACKAGE_NAME=\"libcublas-12-2\" COLAB_KERNEL_MANAGER_PROXY_PORT=\"6000\" CLOUDSDK_PYTHON=\"python3\" NV_LIBNPP_DEV_VERSION=\"12.2.1.4-1\" ENABLE_DIRECTORYPREFETCHER=\"1\" NO_GCE_CHECK=\"False\" JPY_PARENT_PID=\"111\" PYTHONPATH=\"/env/python\" TERM=\"xterm-color\" NV_LIBCUSPARSE_DEV_VERSION=\"12.1.2.141-1\" GIT_PAGER=\"cat\" LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\" NV_CUDNN_VERSION=\"8.9.6.50\" SHLVL=\"0\" PAGER=\"cat\" COLAB_LANGUAGE_SERVER_PROXY=\"/usr/colab/bin/language_service\" NV_CUDA_LIB_VERSION=\"12.2.2-1\" NVARCH=\"x86_64\" NV_CUDNN_PACKAGE_DEV=\"libcudnn8-dev=8.9.6.50-1+cuda12.2\" NV_CUDA_COMPAT_PACKAGE=\"cuda-compat-12-2\" MPLBACKEND=\"module://ipykernel.pylab.backend_inline\" NV_LIBNCCL_PACKAGE=\"libnccl2=2.19.3-1+cuda12.2\" LD_LIBRARY_PATH=\"/usr/lib64-nvidia\" COLAB_GPU=\"1\" GCS_READ_CACHE_BLOCK_SIZE_MB=\"16\" NV_CUDA_NSIGHT_COMPUTE_VERSION=\"12.2.2-1\" NV_NVPROF_VERSION=\"12.2.142-1\" LC_ALL=\"en_US.UTF-8\" COLAB_FILE_HANDLER_ADDR=\"localhost:3453\" PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\" NV_LIBNCCL_PACKAGE_NAME=\"libnccl2\" COLAB_DEBUG_ADAPTER_MUX_PATH=\"/usr/local/bin/dap_multiplexer\" NV_LIBNCCL_PACKAGE_VERSION=\"2.19.3-1\" PYTHONWARNINGS=\"ignore:::pip._internal.cli.base_command\" DEBIAN_FRONTEND=\"noninteractive\" COLAB_BACKEND_VERSION=\"next\" OLDPWD=\"/\" _=\"/usr/local/bin/colossalai\" && torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 opt_benchmark.py --model_name_or_path facebook/opt-125m --plugin gemini --batch_size 128'\n","\n","Exit code: 1\n","\n","Stdout: already printed\n","\n","Stderr: already printed\n","\n","\n","\n","====== Training on All Nodes =====\n","127.0.0.1: failure\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.2"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}