{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712762182258,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"},"user_tz":-480},"id":"mSYAddx3Aohz","outputId":"d27ce9a6-3e4b-45b2-9cdc-d8cdc84aa966"},"outputs":[{"name":"stdout","output_type":"stream","text":["[WinError 2] The system cannot find the file specified: 'Assignment6_ColossalAI_OPT'\n","c:\\Users\\User\\Documents\\Graduate certificate\\CS5260 Neural Network and Deep Learning II\\Assignment6\\OPT\\A0287395J\n"]},{"name":"stderr","output_type":"stream","text":["'git' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!git clone https://github.com/tayyy/Open-Pretrain-Transformer-in-Colossal-AI\n","%cd Open-Pretrain-Transformer-in-Colossal-AI"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12247,"status":"ok","timestamp":1712762197574,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"},"user_tz":-480},"id":"A1H98cHd1all","outputId":"9f22ff2d-66c4-42e7-9e4f-b0a69cdf657a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: colossalai>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.6)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.18.0)\n","Requirement already satisfied: transformers>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.38.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (24.0)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (1.11.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.4.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.6.4)\n","Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.10.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (0.1.99)\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.3.2->-r requirements.txt (line 1)) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r requirements.txt (line 2)) (12.4.127)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->-r requirements.txt (line 4)) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->-r requirements.txt (line 4)) (0.15.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.2.2)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.2.14)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.12.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 2)) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.5.35)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (20.25.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.16.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.0.8)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.1.2)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (42.0.5)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.3.2->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.5)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.3.2->-r requirements.txt (line 1)) (0.18.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.3.2->-r requirements.txt (line 1)) (2.22)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n","Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n"]}],"source":["!set -xe\n","!pip install -r requirements.txt\n","! pip install numba"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5879627,"status":"ok","timestamp":1712758578274,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"},"user_tz":-480},"id":"xfmT8nlTAvqe","outputId":"3720dda4-a702-4025-c3a1-68e999d634a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 12:38:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 12:38:27] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:86 main\n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-350m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:113    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09548354148864746 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10950374603271484 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 12:38:37] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:145    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n","Epoch [1]:   0%|          | 0/550 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","Epoch [1]: 100%|██████████| 550/550 [09:38<00:00,  1.05s/it, loss=1.64]\n","Epoch [2]: 100%|██████████| 550/550 [09:38<00:00,  1.05s/it, loss=1.32]\n","Epoch [3]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=1.08]\n","Epoch [4]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=0.881]\n","Epoch [5]: 100%|██████████| 550/550 [09:36<00:00,  1.05s/it, loss=0.688]\n","Epoch [6]: 100%|██████████| 550/550 [09:51<00:00,  1.07s/it, loss=0.521]\n","Epoch [7]: 100%|██████████| 550/550 [09:46<00:00,  1.07s/it, loss=0.391]\n","Epoch [8]: 100%|██████████| 550/550 [09:52<00:00,  1.08s/it, loss=0.318]\n","Epoch [9]: 100%|██████████| 550/550 [09:56<00:00,  1.08s/it, loss=0.277]\n","Epoch [10]: 100%|██████████| 550/550 [09:56<00:00,  1.08s/it, loss=0.267]\n","[04/10/24 14:16:07] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:150    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n","[04/10/24 14:16:13] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/Assignme\n","                             nt6_ColossalAI_OPT/Assignment6_ColossalAI_OPT/opt_train_demo.py:152    \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n","                             ./output_model.bin                                                     \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}],"source":["\n","\n","# model name or path\n","MODEL=\"facebook/opt-350m\"\n","\n","# path for saving model\n","OUTPUT_PATH=\"./output_model.bin\"\n","\n","# plugin(training strategy)\n","# can only be one of \"torch_ddp\"/\"torch_ddp_fp16\"/\"low_level_zero\"/\"gemini\"\n","PLUGIN=\"gemini\"\n","\n","# number of gpus to use\n","GPUNUM=1\n","\n","# batch size per gpu\n","BS=16\n","\n","# learning rate\n","LR=\"5e-5\"\n","\n","# number of epoch\n","EPOCH=10\n","\n","# weight decay\n","WEIGHT_DECAY=0.01\n","\n","# ratio of warmup steps\n","WARMUP_RATIO=0.1\n","\n","# run the script for demo\n","!colossalai run \\\n","--nproc_per_node {GPUNUM} \\\n","opt_train_demo.py \\\n","--model_name_or_path {MODEL} \\\n","--output_path {OUTPUT_PATH} \\\n","--plugin {PLUGIN} \\\n","--batch_size {BS} \\\n","--num_epoch {EPOCH} \\\n","--learning_rate {LR} \\\n","--weight_decay {WEIGHT_DECAY} \\\n","--warmup_ratio {WARMUP_RATIO}\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207779,"status":"ok","timestamp":1712763273979,"user":{"displayName":"Yee Yang Tay","userId":"02537013327308865879"},"user_tz":-480},"id":"NVU_7psDAxd-","outputId":"3099d8c7-91b6-4e22-dc72-04763a45ac49"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 15:31:13] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 15:31:15] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main           \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main           \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09863734245300293 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.12072038650512695 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 15:31:16] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main           \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [01:01<00:00,  3.12s/it][04/10/24 15:32:18] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main          \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp, throughput: 2.5932, maximum memory usage per gpu:\n","                             8.38 GB.                                                               \n","Training Step: 100%|██████████| 20/20 [01:01<00:00,  3.09s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 15:32:32] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 15:32:35] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main           \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main           \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10242009162902832 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1187753677368164 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 15:32:36] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main           \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:28<00:00,  1.37s/it][04/10/24 15:33:04] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main          \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp_fp16, throughput: 5.7065, maximum memory usage per\n","                             gpu: 7.11 GB.                                                          \n","Training Step: 100%|██████████| 20/20 [00:28<00:00,  1.40s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 15:33:16] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 15:33:19] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main           \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main           \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09904885292053223 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11133289337158203 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/10/24 15:33:20] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main           \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:24<00:00,  1.18s/it][04/10/24 15:33:44] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main          \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: low_level_zero, throughput: 6.5741, maximum memory usage per\n","                             gpu: 4.90 GB.                                                          \n","Training Step: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/10/24 15:33:56] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/10/24 15:33:58] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:68 main           \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             facebook/opt-125m                                                      \n","                    INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:83 main           \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10220789909362793 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1166543960571289 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/10/24 15:34:01] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:96 main           \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.24s/it][04/10/24 15:34:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /content/Assignment6_ColossalAI_OPT/opt_benchmark.py:119 main          \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: gemini, throughput: 6.2924, maximum memory usage per gpu:   \n","                             3.51 GB.                                                               \n","Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}],"source":["from numba import cuda\n","MEMCAP=0\n","GPUNUM=1\n","for BS in [8]:\n","    for PLUGIN in [\"torch_ddp\", \"torch_ddp_fp16\", \"low_level_zero\", \"gemini\"]:\n","        MODLE_PATH=\"facebook/opt-125m\"\n","        !colossalai run \\\n","        --nproc_per_node {GPUNUM} \\\n","        opt_benchmark.py \\\n","        --model_name_or_path {MODLE_PATH} \\\n","        --mem_cap {MEMCAP} \\\n","        --plugin {PLUGIN} \\\n","        --batch_size {BS}\n","        device = cuda.get_current_device()\n","        device.reset()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":0}
